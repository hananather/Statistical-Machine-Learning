\newpage
\section{January 18, 2022}
\term{What is learning algorithm?} \\
A \textbf{supervised learning algorithm} $A$ is a way of choosing $f \in \SF$ based on data $\bar{z} \in Z^n$. We can view the learning algorithm as a map
$A: Z^n \to \SF$, where $A: \bar{z} = (z_1, \dots, z_n) \mapsto f_{\bar{z}, A}$. And evaluate learning algorithms, $A$, by how much they can minimize $\SR(f)$.
\begin{definition}[Bayes Error, Best in class]
    There are essentially three levels at which we can view our function $f$ at: $f_*, f_{\SF}, f_{\SF,n A} = A(\bar{z})$.
    \begin{itemize}
        \item $f_*:$ the function that minimizes the risk $\SR(f)= \EE_{p}V(f,z)$ over the class of all any measurable  function over the distribution $P$. And $\SR(f_{*})$ is the \vocab{Bayes error.}

        \item $f_{\SF}$: the function that minimizes risk $\SR(f)$ over some \textit{restricted class} $\SF$ of measurable functions for the distribution $P$. \textbf{$A$ only operates within $\SF$}.

        \item $f_{\SF,n A} = A(\bar{z})$: the function produced by the algorithm A, on sample $\bar{z} = (z_1, \dots, z_n)$ from $P^n$ \footnote{in fact this is an abuse of notation and $f_{\SF,\bar{z}, A}$ would be more precise  since the algorithm sees a specific sample $\bar{z}$. However, the notation is emphasizing the fact that the function depends on the algorithm $A$ handing a $n-$sample from $P$.}
    \end{itemize}
\end{definition}
\begin{itemize}
    \item \textbf{generalization error} = $\SR(f_{\SF,n A}) - \SR(f_*)$
    \item \textbf{estimation error} = $\SR(f_{\SF,n A}) - \SR(f_\SF)$
    \item \textbf{approximation error} = $\SR(f_\SF) - \SR(f_*)$
    
\end{itemize}
$$
\text{generalization error = estimation error + approximation error}
$$
\subsection{Bayes Optimal}
\begin{proposition}
    Given $V(f,z) = (f(x) - y)^2$, $f_* = \EE_p[y|x]$ is the \vocab{Bayes optimal}..
\end{proposition}